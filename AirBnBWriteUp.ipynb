{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                             AirBnb Price Predictions\n",
    "\n",
    "    The dataset I decided to work with for this MATH421 project was the 'AirBnB listings in major U.S cities' dataset off of kaggle. The dataset was used for the 'Deloitte Machine Learning competition' and posted by user RudyMizrahi. So the major variable I saw as obvious to predict was the price of each listing. Right away, I had to raise the log_price to the power of e to get the real prices. I thought it'd be interesting to to look at the distribution of these real prices first, along with locating the mean of about 160. This gave me the starting point of what to do with this target variable, split it into prices that were <160 and >160, dollars I'm assuming.\n",
    "    After this I started looking at the levels of each variable and the variables in general. Of the 29 variables, and 74,111 observations, I had to cut down on the variables and locate missing values. I elimintated: Last review date, id#, thumbnail URL, host since [date], cancellation policy, description, neighbourhood, name, first review, zipcode, latitude, longitude, and amenities(too many levels). I cut down the levels of categorical variables including: host response rate (100, not 100), bed type (bed, not a bed), property type (house, apartment, other), identity verified (T/F), profile pic of host (T/F), instant bookable(T/F).\n",
    "    To start by talking about some of the graphs, the first one I plotted was a bar graph of the target variable, \"log_price\", which is really \"<160\" or \">160\". About 68.9 percent of the listings were <160. The next graph, I plotted <160 vs. >160, filled by property_type, and a bigger portion of <160 consisted of apartments, than >160. Then we have the target, filled by room_type, and most of the listings >160 are for an entire home/apt, while most of the <160 are a private room. I plotted the bar graph of the target filled by # of beds, and it seems that there are still rooms with more than 10 beds for <160, which wasn't expected. Taking a closer look, the subset of the data containing only # of bed > or = 5, still shows no pattern on >160 and <160. Next, log price and # of bedrooms, a very big portion of the listings for <160 are 1 bedroom. Switching the order of # of bedrooms and log_price, in a density graph, we see that right around 2 beds, we start see that the probability of the price being <160 is very close to >160. Many of the hosts that don't have a confirmed identity, have profile pics; scammers?; Whatever it is, profile pic doesn't determine if the host can be idenitifed. For 1 bedroom, it's very likely to be <160, but once you have 2 bedroom, you'll probably be paying >160. There is no relation between number of reviews to pricing. Looking at the major cities vs. price, for Boston and San Fran, it looks like you're more likely to pay >160, compared to NYC, LA, DC, and Chicago. Number of reviews and review scores has little relationship. The relationship between # of beds and bedrooms is relatively postive which is what I expected, but you still see some zero bedroom listings with 15 beds, which is odd. According to this next graph, it looks like if you have a 100% response rate, the number of reviews has a stronger chance of leading to a good review score. Looking at three variables, room_type on the x, filled by property_type, and controlling for the target; I expected to see less \"entire home/apt\" listings for the <160 chart, and I also expected to see less apartments for the \"entire home/apt\"\" listings at the >160 level.\n",
    "    The next part of the model that required some attention, was handling missing values. For this, I chose to use the mean and mode method, the knnImpute method, and the na.omit method(gets rid of rows with missing values). The knnImpute method didn't seem to work too well, as it was taking a while to run, possibly due to lack of computing power. I was able to compare the results of na.omit and mean/mode throughout the different models though. After imputing those different forms of NA, I trained a model for each, by partioning the data of 70% training and 30% for testing. I trained those 2 models (one mean/mode and one na.omit) for a Caret model, and tested them on their respective testing datasets. The accuracy of the mean/mode was .7218 with a balanced accuracy of .5, and na.omit's results were .7221 and .50525, respectively.\n",
    "    I decided to recode the categorical variables with dummyVars and it increased the balanced accuracy of my models tremendously. Performing another Caret model, for mean/mode with dummyVars, the accuracy and balanced accuracy were .802 and .725. The same numbers for na.omit were, .822 and .754. So na.omit and dummyVars are substantially better so far, and I decided to build the rest of my models with those two possibiliities, mean/mode dummyVars, and na.omit dummyVars.\n",
    "    The other types of models I created include, a Ranger Random forest, a 3 fold cross validation with Ranger, and a Tuned model. Training two of each of these, I came up with 6 more models. Looking at the Ranger Random forest, mean/mode with dummyVars produced an accuracy of .824 and balanced accuracy of .767. Na.omit for the same model, produced .8357 and .774, respectively. \n",
    "    The 3 fold cross-validation w/ Ranger had mixed results. The mean/mode results had the highest accuracy at 2 mtry and coming from the gini method; accuracy = .817. Then for the na.omit method, the highest accuracy comes at the same point again, mtry = 2, gini method; accuracy = .827\n",
    "    The last model I decided to create was a Tuned model for each of those 2 refined datasets. For the mean/mode with dummyVars Tuned model, the highest accuracy rate comes at 4 randomly selected parameters, regardless of node size with the gini method; accuracy = .82. As for the na.omit with dummyVars Tuned model, the highest accuracy comes at the same point; gini method with 4 randomly selected parameters, regardless of node size; accuracy = .835."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
